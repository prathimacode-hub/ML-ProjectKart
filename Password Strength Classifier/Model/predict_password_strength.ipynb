{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Password Strength Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the datasets we will be having a varioid passwords and their strength and we need to classify and predict the strength of the given password , either strong, weak or average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Password Strength Classifier we will use 3 algorithms:\n",
    "\n",
    "1.Linear Regression\n",
    "\n",
    "2.Ridge Regression\n",
    "\n",
    "3.Decision Tree Regressor\n",
    "\n",
    "By using the above algorithms, we will train our model by providing training data and once the model will be trained, we will perform prediction. After prediction, we will evaluate the performance of these algorithmns by error check and accuracy check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps been followed are as:\n",
    "\n",
    "Step 1:Data Exploration\n",
    "\n",
    "Step 2: Data Preparation\n",
    "\n",
    "Step 3: Data Visualization\n",
    "\n",
    "Step 4:Data training\n",
    "\n",
    "Step 5: Model Creation\n",
    "\n",
    "Step 6: Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  first we need to read the data from data.csv file, therefore we need to import the basic python library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns # we can also use matplotlib for visulaization purpose\n",
    "import warnings  # to get rid of all the warnings that come across the cell, we import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2810: expected 2 fields, saw 5\\nSkipping line 4641: expected 2 fields, saw 5\\nSkipping line 7171: expected 2 fields, saw 5\\nSkipping line 11220: expected 2 fields, saw 5\\nSkipping line 13809: expected 2 fields, saw 5\\nSkipping line 14132: expected 2 fields, saw 5\\nSkipping line 14293: expected 2 fields, saw 5\\nSkipping line 14865: expected 2 fields, saw 5\\nSkipping line 17419: expected 2 fields, saw 5\\nSkipping line 22801: expected 2 fields, saw 5\\nSkipping line 25001: expected 2 fields, saw 5\\nSkipping line 26603: expected 2 fields, saw 5\\nSkipping line 26742: expected 2 fields, saw 5\\nSkipping line 29702: expected 2 fields, saw 5\\nSkipping line 32767: expected 2 fields, saw 5\\nSkipping line 32878: expected 2 fields, saw 5\\nSkipping line 35643: expected 2 fields, saw 5\\nSkipping line 36550: expected 2 fields, saw 5\\nSkipping line 38732: expected 2 fields, saw 5\\nSkipping line 40567: expected 2 fields, saw 5\\nSkipping line 40576: expected 2 fields, saw 5\\nSkipping line 41864: expected 2 fields, saw 5\\nSkipping line 46861: expected 2 fields, saw 5\\nSkipping line 47939: expected 2 fields, saw 5\\nSkipping line 48628: expected 2 fields, saw 5\\nSkipping line 48908: expected 2 fields, saw 5\\nSkipping line 57582: expected 2 fields, saw 5\\nSkipping line 58782: expected 2 fields, saw 5\\nSkipping line 58984: expected 2 fields, saw 5\\nSkipping line 61518: expected 2 fields, saw 5\\nSkipping line 63451: expected 2 fields, saw 5\\nSkipping line 68141: expected 2 fields, saw 5\\nSkipping line 72083: expected 2 fields, saw 5\\nSkipping line 74027: expected 2 fields, saw 5\\nSkipping line 77811: expected 2 fields, saw 5\\nSkipping line 83958: expected 2 fields, saw 5\\nSkipping line 85295: expected 2 fields, saw 5\\nSkipping line 88665: expected 2 fields, saw 5\\nSkipping line 89198: expected 2 fields, saw 5\\nSkipping line 92499: expected 2 fields, saw 5\\nSkipping line 92751: expected 2 fields, saw 5\\nSkipping line 93689: expected 2 fields, saw 5\\nSkipping line 94776: expected 2 fields, saw 5\\nSkipping line 97334: expected 2 fields, saw 5\\nSkipping line 102316: expected 2 fields, saw 5\\nSkipping line 103421: expected 2 fields, saw 5\\nSkipping line 106872: expected 2 fields, saw 5\\nSkipping line 109363: expected 2 fields, saw 5\\nSkipping line 110117: expected 2 fields, saw 5\\nSkipping line 110465: expected 2 fields, saw 5\\nSkipping line 113843: expected 2 fields, saw 5\\nSkipping line 115634: expected 2 fields, saw 5\\nSkipping line 121518: expected 2 fields, saw 5\\nSkipping line 123692: expected 2 fields, saw 5\\nSkipping line 124708: expected 2 fields, saw 5\\nSkipping line 129608: expected 2 fields, saw 5\\nSkipping line 133176: expected 2 fields, saw 5\\nSkipping line 135532: expected 2 fields, saw 5\\nSkipping line 138042: expected 2 fields, saw 5\\nSkipping line 139485: expected 2 fields, saw 5\\nSkipping line 140401: expected 2 fields, saw 5\\nSkipping line 144093: expected 2 fields, saw 5\\nSkipping line 149850: expected 2 fields, saw 5\\nSkipping line 151831: expected 2 fields, saw 5\\nSkipping line 158014: expected 2 fields, saw 5\\nSkipping line 162047: expected 2 fields, saw 5\\nSkipping line 164515: expected 2 fields, saw 5\\nSkipping line 170313: expected 2 fields, saw 5\\nSkipping line 171325: expected 2 fields, saw 5\\nSkipping line 171424: expected 2 fields, saw 5\\nSkipping line 175920: expected 2 fields, saw 5\\nSkipping line 176210: expected 2 fields, saw 5\\nSkipping line 183603: expected 2 fields, saw 5\\nSkipping line 190264: expected 2 fields, saw 5\\nSkipping line 191683: expected 2 fields, saw 5\\nSkipping line 191988: expected 2 fields, saw 5\\nSkipping line 195450: expected 2 fields, saw 5\\nSkipping line 195754: expected 2 fields, saw 5\\nSkipping line 197124: expected 2 fields, saw 5\\nSkipping line 199263: expected 2 fields, saw 5\\nSkipping line 202603: expected 2 fields, saw 5\\nSkipping line 209960: expected 2 fields, saw 5\\nSkipping line 213218: expected 2 fields, saw 5\\nSkipping line 217060: expected 2 fields, saw 5\\nSkipping line 220121: expected 2 fields, saw 5\\nSkipping line 223518: expected 2 fields, saw 5\\nSkipping line 226293: expected 2 fields, saw 5\\nSkipping line 227035: expected 2 fields, saw 7\\nSkipping line 227341: expected 2 fields, saw 5\\nSkipping line 227808: expected 2 fields, saw 5\\nSkipping line 228516: expected 2 fields, saw 5\\nSkipping line 228733: expected 2 fields, saw 5\\nSkipping line 232043: expected 2 fields, saw 5\\nSkipping line 232426: expected 2 fields, saw 5\\nSkipping line 234490: expected 2 fields, saw 5\\nSkipping line 239626: expected 2 fields, saw 5\\nSkipping line 240461: expected 2 fields, saw 5\\nSkipping line 244518: expected 2 fields, saw 5\\nSkipping line 245395: expected 2 fields, saw 5\\nSkipping line 246168: expected 2 fields, saw 5\\nSkipping line 246655: expected 2 fields, saw 5\\nSkipping line 246752: expected 2 fields, saw 5\\nSkipping line 247189: expected 2 fields, saw 5\\nSkipping line 250276: expected 2 fields, saw 5\\nSkipping line 255327: expected 2 fields, saw 5\\nSkipping line 257094: expected 2 fields, saw 5\\n'\n",
      "b'Skipping line 264626: expected 2 fields, saw 5\\nSkipping line 265028: expected 2 fields, saw 5\\nSkipping line 269150: expected 2 fields, saw 5\\nSkipping line 271360: expected 2 fields, saw 5\\nSkipping line 273975: expected 2 fields, saw 5\\nSkipping line 274742: expected 2 fields, saw 5\\nSkipping line 276227: expected 2 fields, saw 5\\nSkipping line 279807: expected 2 fields, saw 5\\nSkipping line 283425: expected 2 fields, saw 5\\nSkipping line 287468: expected 2 fields, saw 5\\nSkipping line 292995: expected 2 fields, saw 5\\nSkipping line 293496: expected 2 fields, saw 5\\nSkipping line 293735: expected 2 fields, saw 5\\nSkipping line 295060: expected 2 fields, saw 5\\nSkipping line 296643: expected 2 fields, saw 5\\nSkipping line 296848: expected 2 fields, saw 5\\nSkipping line 308926: expected 2 fields, saw 5\\nSkipping line 310360: expected 2 fields, saw 5\\nSkipping line 317004: expected 2 fields, saw 5\\nSkipping line 318207: expected 2 fields, saw 5\\nSkipping line 331783: expected 2 fields, saw 5\\nSkipping line 333864: expected 2 fields, saw 5\\nSkipping line 335958: expected 2 fields, saw 5\\nSkipping line 336290: expected 2 fields, saw 5\\nSkipping line 343526: expected 2 fields, saw 5\\nSkipping line 343857: expected 2 fields, saw 5\\nSkipping line 344059: expected 2 fields, saw 5\\nSkipping line 348691: expected 2 fields, saw 5\\nSkipping line 353446: expected 2 fields, saw 5\\nSkipping line 357073: expected 2 fields, saw 5\\nSkipping line 359753: expected 2 fields, saw 5\\nSkipping line 359974: expected 2 fields, saw 5\\nSkipping line 366534: expected 2 fields, saw 5\\nSkipping line 369514: expected 2 fields, saw 5\\nSkipping line 377759: expected 2 fields, saw 5\\nSkipping line 379327: expected 2 fields, saw 5\\nSkipping line 380769: expected 2 fields, saw 5\\nSkipping line 381073: expected 2 fields, saw 5\\nSkipping line 381489: expected 2 fields, saw 5\\nSkipping line 386304: expected 2 fields, saw 5\\nSkipping line 387635: expected 2 fields, saw 5\\nSkipping line 389613: expected 2 fields, saw 5\\nSkipping line 392604: expected 2 fields, saw 5\\nSkipping line 393184: expected 2 fields, saw 5\\nSkipping line 395530: expected 2 fields, saw 5\\nSkipping line 396939: expected 2 fields, saw 5\\nSkipping line 397385: expected 2 fields, saw 5\\nSkipping line 397509: expected 2 fields, saw 5\\nSkipping line 402902: expected 2 fields, saw 5\\nSkipping line 405187: expected 2 fields, saw 5\\nSkipping line 408412: expected 2 fields, saw 5\\nSkipping line 419423: expected 2 fields, saw 5\\nSkipping line 420962: expected 2 fields, saw 5\\nSkipping line 425965: expected 2 fields, saw 5\\nSkipping line 427496: expected 2 fields, saw 5\\nSkipping line 438881: expected 2 fields, saw 5\\nSkipping line 439776: expected 2 fields, saw 5\\nSkipping line 440345: expected 2 fields, saw 5\\nSkipping line 445507: expected 2 fields, saw 5\\nSkipping line 445548: expected 2 fields, saw 5\\nSkipping line 447184: expected 2 fields, saw 5\\nSkipping line 448603: expected 2 fields, saw 5\\nSkipping line 451732: expected 2 fields, saw 5\\nSkipping line 458249: expected 2 fields, saw 5\\nSkipping line 460274: expected 2 fields, saw 5\\nSkipping line 467630: expected 2 fields, saw 5\\nSkipping line 473961: expected 2 fields, saw 5\\nSkipping line 476281: expected 2 fields, saw 5\\nSkipping line 478010: expected 2 fields, saw 5\\nSkipping line 478322: expected 2 fields, saw 5\\nSkipping line 479999: expected 2 fields, saw 5\\nSkipping line 480898: expected 2 fields, saw 5\\nSkipping line 481688: expected 2 fields, saw 5\\nSkipping line 485193: expected 2 fields, saw 5\\nSkipping line 485519: expected 2 fields, saw 5\\nSkipping line 486000: expected 2 fields, saw 5\\nSkipping line 489063: expected 2 fields, saw 5\\nSkipping line 494525: expected 2 fields, saw 5\\nSkipping line 495009: expected 2 fields, saw 5\\nSkipping line 501954: expected 2 fields, saw 5\\nSkipping line 508035: expected 2 fields, saw 5\\nSkipping line 508828: expected 2 fields, saw 5\\nSkipping line 509833: expected 2 fields, saw 5\\nSkipping line 510410: expected 2 fields, saw 5\\nSkipping line 518229: expected 2 fields, saw 5\\nSkipping line 520302: expected 2 fields, saw 5\\nSkipping line 520340: expected 2 fields, saw 5\\n'\n",
      "b'Skipping line 525174: expected 2 fields, saw 5\\nSkipping line 526251: expected 2 fields, saw 5\\nSkipping line 529611: expected 2 fields, saw 5\\nSkipping line 531398: expected 2 fields, saw 5\\nSkipping line 534146: expected 2 fields, saw 5\\nSkipping line 544954: expected 2 fields, saw 5\\nSkipping line 553002: expected 2 fields, saw 5\\nSkipping line 553883: expected 2 fields, saw 5\\nSkipping line 553887: expected 2 fields, saw 5\\nSkipping line 553915: expected 2 fields, saw 5\\nSkipping line 554172: expected 2 fields, saw 5\\nSkipping line 563534: expected 2 fields, saw 5\\nSkipping line 565191: expected 2 fields, saw 5\\nSkipping line 574108: expected 2 fields, saw 5\\nSkipping line 574412: expected 2 fields, saw 5\\nSkipping line 575985: expected 2 fields, saw 5\\nSkipping line 580091: expected 2 fields, saw 5\\nSkipping line 582682: expected 2 fields, saw 5\\nSkipping line 585885: expected 2 fields, saw 5\\nSkipping line 590171: expected 2 fields, saw 5\\nSkipping line 591924: expected 2 fields, saw 5\\nSkipping line 592515: expected 2 fields, saw 5\\nSkipping line 593888: expected 2 fields, saw 5\\nSkipping line 596245: expected 2 fields, saw 5\\nSkipping line 607344: expected 2 fields, saw 5\\nSkipping line 607633: expected 2 fields, saw 5\\nSkipping line 610939: expected 2 fields, saw 5\\nSkipping line 613638: expected 2 fields, saw 5\\nSkipping line 615643: expected 2 fields, saw 5\\nSkipping line 615901: expected 2 fields, saw 5\\nSkipping line 617389: expected 2 fields, saw 5\\nSkipping line 634641: expected 2 fields, saw 5\\nSkipping line 635755: expected 2 fields, saw 5\\nSkipping line 646243: expected 2 fields, saw 5\\nSkipping line 647165: expected 2 fields, saw 5\\nSkipping line 648610: expected 2 fields, saw 5\\nSkipping line 648772: expected 2 fields, saw 5\\nSkipping line 651833: expected 2 fields, saw 5\\nSkipping line 653663: expected 2 fields, saw 5\\nSkipping line 656233: expected 2 fields, saw 5\\nSkipping line 656694: expected 2 fields, saw 5\\nSkipping line 659783: expected 2 fields, saw 5\\nSkipping line 660478: expected 2 fields, saw 5\\nSkipping line 661133: expected 2 fields, saw 5\\nSkipping line 661736: expected 2 fields, saw 5\\nSkipping line 669827: expected 2 fields, saw 5\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kzde5577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kino3434</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>visi7k1yr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>megzy123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lamborghin1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      password  strength\n",
       "0     kzde5577         1\n",
       "1     kino3434         1\n",
       "2    visi7k1yr         1\n",
       "3     megzy123         1\n",
       "4  lamborghin1         1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READING THE DATASETS\n",
    "data=pd.read_csv('../Dataset/data.csv',error_bad_lines=False)# read the data using read_csv function and setting the value of error_bad_lines = false, because it is by default true\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['strength'].unique() #Checking the unique strength present in dataset, 0-poor, 1 for average, 2 for best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "password    1\n",
       "strength    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECKING ALL THE MISSING VALUES IN DATASET AND DROPPOING THEM ALL\n",
    "data.isna().sum() # checking is there any Nan value in data, here only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367579</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       password  strength\n",
       "367579      NaN         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['password'].isnull()] # finding the position where it is Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True) # dropping that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "password    0\n",
       "strength    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() # after dropping , checking if there is not any Nan value, here 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='strength', ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['strength']) # checking the freq of each category of strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "password_tuple=np.array(data) # now we create an array containing all the data of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['kzde5577', 1],\n",
       "       ['kino3434', 1],\n",
       "       ['visi7k1yr', 1],\n",
       "       ...,\n",
       "       ['184520socram', 1],\n",
       "       ['marken22a', 1],\n",
       "       ['fxx4pw4g', 1]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "password_tuple # printing that array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['kzde5577', 1],\n",
       "       ['kino3434', 1],\n",
       "       ['visi7k1yr', 1],\n",
       "       ...,\n",
       "       ['duong12', 0],\n",
       "       ['tendy1992', 1],\n",
       "       ['vikramvicky16', 1]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling the data to create the robustness\n",
    "import random # therefore importing random\n",
    "random.shuffle(password_tuple) # using shuffle function make the array shuffled\n",
    "password_tuple # printing the array after shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for list comprehension\n",
    "# first column is put in x list\n",
    "# ans 2nd column in y list\n",
    "x=[labels[0] for labels in password_tuple]\n",
    "y=[labels[1] for labels in password_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kzde5577',\n",
       " 'kino3434',\n",
       " 'visi7k1yr',\n",
       " 'kino3434',\n",
       " 'visi7k1yr',\n",
       " 'megzy123',\n",
       " 'u6c8vhow',\n",
       " 'v1118714',\n",
       " 'v1118714',\n",
       " 'megzy123',\n",
       " 'kzde5577',\n",
       " 'u6c8vhow',\n",
       " 'universe2908',\n",
       " 'v1118714',\n",
       " 'asv5o9yu',\n",
       " 'universe2908',\n",
       " 'jerusalem393',\n",
       " '612035180tok',\n",
       " 'idofo673',\n",
       " 'sbl571017',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'jerusalem393',\n",
       " 'jytifok873',\n",
       " 'kzde5577',\n",
       " 'idofo673',\n",
       " 'cigicigi123',\n",
       " 'klara-tershina3H',\n",
       " 'jytifok873',\n",
       " 'asv5o9yu',\n",
       " 'g067057895',\n",
       " 'klara-tershina3H',\n",
       " 'u6c8vhow',\n",
       " 'lamborghin1',\n",
       " '612035180tok',\n",
       " 'lamborghin1',\n",
       " 'jytifok873',\n",
       " '612035180tok',\n",
       " '52558000aaa',\n",
       " 'memjan123',\n",
       " 'asgaliu11',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'fk9qi21m',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'universe2908',\n",
       " 'v1118714',\n",
       " '612035180tok',\n",
       " 'fk9qi21m',\n",
       " '6975038lp',\n",
       " 'yitbos77',\n",
       " 'kino3434',\n",
       " 'asv5o9yu',\n",
       " 'asgaliu11',\n",
       " 'yitbos77',\n",
       " 'g067057895',\n",
       " 'ejeko677',\n",
       " '0169395484a',\n",
       " 'ejeko677',\n",
       " 'tamanagung6',\n",
       " 'c3h8bkzr',\n",
       " 'openup12',\n",
       " 'exitos2009',\n",
       " 'megzy123',\n",
       " 'as326159',\n",
       " 'ejeko677',\n",
       " 'schalke04',\n",
       " 'u6c8vhow',\n",
       " 'memjan123',\n",
       " 'bgrvl80',\n",
       " 'as326159',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'yitbos77',\n",
       " 'ejeko677',\n",
       " 'bgrvl80',\n",
       " '612035180tok',\n",
       " 'megzy123',\n",
       " 'megzy123',\n",
       " 'prisonbreak1',\n",
       " 'khmer100.03278&?><Mnb',\n",
       " 'lamborghin1',\n",
       " 'bgrvl80',\n",
       " 'klara-tershina3H',\n",
       " 'trabajonet9',\n",
       " 'jytifok873',\n",
       " 'c3h8bkzr',\n",
       " 'elyass15@ajilent-ci',\n",
       " 'gdfn76',\n",
       " 'idofo673',\n",
       " 'mickael12',\n",
       " 'Iamthelegend1!',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'sknq7m0',\n",
       " 'jytifok873',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'cigicigi123',\n",
       " 'k1k2k3k4k5k6',\n",
       " 'TyWM72UNEex8Q8Y',\n",
       " 'k9b8cz6aj2',\n",
       " 'gill02',\n",
       " '838188linh',\n",
       " 'il0vey0u',\n",
       " '2021848709.',\n",
       " 'yllime123',\n",
       " '6975038lp',\n",
       " 'fk9qi21m',\n",
       " 'juliel009',\n",
       " 'hayhayq2',\n",
       " 'visi7k1yr',\n",
       " 'bgrvl80',\n",
       " 'lamborghin1',\n",
       " '0169395484a',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'bgrvl80',\n",
       " 'ikanez886',\n",
       " 'il0vey0u',\n",
       " 'asgaliu11',\n",
       " '52558000aaa',\n",
       " 'bozoxik602',\n",
       " 'teemteem97',\n",
       " 'lsdlsd1',\n",
       " 'gaymaids1',\n",
       " 'mmm23mm',\n",
       " 'alimagik1',\n",
       " 'icap12',\n",
       " '283671gus',\n",
       " '64959rodro',\n",
       " 'jerusalem393',\n",
       " 'openup12',\n",
       " 'k9b8cz6aj2',\n",
       " 'idofo673',\n",
       " 'tamanagung6',\n",
       " 'juliana19',\n",
       " '52558000aaa',\n",
       " '123477889a',\n",
       " 'ns2b0727',\n",
       " 's4m2dx9e6',\n",
       " 'jonothepoop1',\n",
       " 'farrukhcse12',\n",
       " 'calcifer32',\n",
       " 'bozoxik602',\n",
       " 'olmaz.',\n",
       " 'vehat387',\n",
       " 'mickael12',\n",
       " 'universe2908',\n",
       " 'k1k2k3k4k5k6',\n",
       " 'nicolas05',\n",
       " 'a2531106',\n",
       " 'juliel009',\n",
       " 'cigicigi123',\n",
       " 'zoobike04',\n",
       " 'jytifok873',\n",
       " '123net123',\n",
       " 'calcifer32',\n",
       " 'hpqkoxsn5',\n",
       " '147963asd',\n",
       " 'g3rappa',\n",
       " '2010server',\n",
       " 'woon12',\n",
       " 'www32223222',\n",
       " 'warriors08',\n",
       " 'potatobus150',\n",
       " '64959rodro',\n",
       " 'bgrvl80',\n",
       " 'fnmsdha476',\n",
       " '52558000aaa',\n",
       " 'bozoxik602',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'klara-tershina3H',\n",
       " 'asgaliu11',\n",
       " 'gill02',\n",
       " 'IjUcOtYqAwel725',\n",
       " 'cigicigi123',\n",
       " 'z3ro1sm',\n",
       " 'p2share',\n",
       " 'intel1',\n",
       " 'g3rappa',\n",
       " 'gill02',\n",
       " 'prisonbreak1',\n",
       " 'hodygid757',\n",
       " 'kunyukbabi69',\n",
       " '283671gus',\n",
       " '746xitEGiqObog',\n",
       " 'juliel009',\n",
       " 'elonex24',\n",
       " 'jonothepoop1',\n",
       " 'klara-tershina3H',\n",
       " 'p@sslng2diword',\n",
       " 'yut0838828185',\n",
       " 'snolyuj04',\n",
       " 'marita1',\n",
       " 'fk9qi21m',\n",
       " '3vszncp4',\n",
       " 'jonothepoop1',\n",
       " 'kjkjkj1',\n",
       " 'potatobus150',\n",
       " 'portales1',\n",
       " '6975038lp',\n",
       " 'sanki1',\n",
       " 'omakiva153',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'as326159',\n",
       " 'teemteem97',\n",
       " 'sanki1',\n",
       " 'jerusalem393',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " '929865yt',\n",
       " 'g067057895',\n",
       " 'alimagik1',\n",
       " 'yu4cmn',\n",
       " '929865yt',\n",
       " '10Erjrlmebup0n',\n",
       " 'hpqkoxsn5',\n",
       " 'nicolas05',\n",
       " 'z7zbgIDkzMQeHUd9',\n",
       " '159951josh',\n",
       " 'ginger972',\n",
       " 'tomas7896',\n",
       " 'ldteugao6',\n",
       " 'kjkjkj1',\n",
       " 'visi7k1yr',\n",
       " '26522876p',\n",
       " '52558000aaa',\n",
       " 'sasuke4',\n",
       " 'g3rappa',\n",
       " 'c3h8bkzr',\n",
       " 'lamborghin1',\n",
       " 'franczuk33',\n",
       " 'vehat387',\n",
       " 'ass359',\n",
       " 'obstacle25',\n",
       " 'asgaliu11',\n",
       " 'barra23',\n",
       " 'pablo321159',\n",
       " 'pato221182',\n",
       " 'jerusalem393',\n",
       " 'ubojig109',\n",
       " 'jalingo1',\n",
       " '4fqa52vecr',\n",
       " 'xyws951753',\n",
       " 'potatobus150',\n",
       " 'snolyuj04',\n",
       " 'tamanagung6',\n",
       " '12345yolanda',\n",
       " 'gaymaids1',\n",
       " 'koabcswzt3',\n",
       " 'hpqkoxsn5',\n",
       " 'elonex24',\n",
       " 's9830950044',\n",
       " 'afs34214',\n",
       " 'yut0838828185',\n",
       " 'idofo673',\n",
       " 'asv5o9yu',\n",
       " '159951josh',\n",
       " 'gdfn76',\n",
       " '123maxbala',\n",
       " 'yqugu927',\n",
       " 'ok>bdk',\n",
       " '746xitEGiqObog',\n",
       " 'ejeko677',\n",
       " 'polo2014',\n",
       " 'jonothepoop1',\n",
       " 'ajyrew547',\n",
       " 'change201',\n",
       " 'teemteem97',\n",
       " 'kino3434',\n",
       " 'rogyh820',\n",
       " '7942vikas',\n",
       " 'k9b8cz6aj2',\n",
       " 'moken7',\n",
       " '26522876p',\n",
       " 'a2486315',\n",
       " 's4m2dx9e6',\n",
       " 'z888888',\n",
       " 'obstacle25',\n",
       " 'ram@!sita15392',\n",
       " 'ldteugao6',\n",
       " '2fakjv',\n",
       " '26522876p',\n",
       " 'denise18',\n",
       " 'jalal123456',\n",
       " 'tin030201',\n",
       " '283671gus',\n",
       " 'y0unus',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " '2yz4ewwg',\n",
       " 'owote852',\n",
       " 'a03242241431a',\n",
       " 's4m2dx9e6',\n",
       " 'koabcswzt3',\n",
       " 'xW8-3w7-MFB-CKH',\n",
       " 'ekufite742',\n",
       " 'jalingo1',\n",
       " 'sydney213',\n",
       " 'kikeq102',\n",
       " 'barboza221294',\n",
       " 'universe2908',\n",
       " 'universe2908',\n",
       " 'rogyh820',\n",
       " 'QWERTY0011',\n",
       " 'cigicigi123',\n",
       " 'witek1709',\n",
       " 'jytifok873',\n",
       " '1katertje',\n",
       " 'IjUcOtYqAwel725',\n",
       " 'kikeq102',\n",
       " 'hosna1368',\n",
       " 'X9WVojjE4MgVAIiR',\n",
       " 'den019520',\n",
       " '123477889a',\n",
       " 'afs34214',\n",
       " 'alchimie79',\n",
       " 'wibi182d',\n",
       " 'demon10',\n",
       " 'universe2908',\n",
       " '4osxw4r',\n",
       " 'warriors08',\n",
       " 'alchimie79',\n",
       " 'patty94',\n",
       " 'trabajonet9',\n",
       " 'kzde5577',\n",
       " '7mV0pKTA3MgHy8Jv',\n",
       " 'tin030201',\n",
       " 'asv5o9yu',\n",
       " 'rogyh820',\n",
       " 'mustang337',\n",
       " 'w9209640',\n",
       " 'mike09',\n",
       " 'mazdarx7',\n",
       " 'caramelo9',\n",
       " 'xanyrum650',\n",
       " 'kenneth610',\n",
       " '159951josh',\n",
       " 'ekufite742',\n",
       " 'patty94',\n",
       " 'pugceya468',\n",
       " 'zidadoh258',\n",
       " 'hasan18',\n",
       " 'uxyloga692',\n",
       " 'hqh2eYjQxOQPYIsA',\n",
       " 'kzde5577',\n",
       " 'rntprns7',\n",
       " 'cesarmaio1',\n",
       " '147963asd',\n",
       " 'cUFUSYKIPuGo024',\n",
       " 'okn9zp9o',\n",
       " 'meopvywk628',\n",
       " 'icap12',\n",
       " 'w9209640',\n",
       " 'jytifok873',\n",
       " 'wibi182d',\n",
       " 'pato221182',\n",
       " 'cigicigi123',\n",
       " 'meopvywk628',\n",
       " 'kunyukbabi69',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'poseidon2011',\n",
       " 'uxyloga692',\n",
       " 'a0972986650',\n",
       " 'trabajonet9',\n",
       " 'hodygid757',\n",
       " 'kzde5577',\n",
       " 'vgnfs495vp',\n",
       " 'acetita478',\n",
       " 'tahseen75',\n",
       " 'e667794c1d',\n",
       " '1katertje',\n",
       " 'wisal1234',\n",
       " 'calcifer32',\n",
       " 'megzy123',\n",
       " 'p2share',\n",
       " 'fbjurcd961',\n",
       " 'jUV4dSDQwNwPpA36',\n",
       " '20010509wang',\n",
       " 'afs34214',\n",
       " 'czuodhj972',\n",
       " 'olmaz.',\n",
       " 'mike09',\n",
       " 'gvczfel801',\n",
       " 'gandhi8513',\n",
       " '123477889a',\n",
       " '33kanun03',\n",
       " 'aio42fv',\n",
       " 'pugceya468',\n",
       " 'k9b8cz6aj2',\n",
       " 'kjkjkj1',\n",
       " 'alchimie79',\n",
       " 'il0vey0u',\n",
       " 'denise18',\n",
       " 'z7zbgIDkzMQeHUd9',\n",
       " 'llahetihw1',\n",
       " 'memjan123',\n",
       " 'cockw0mble',\n",
       " '1ngaymuadong',\n",
       " 'pato221182',\n",
       " 'prisonbreak1',\n",
       " 'ass359',\n",
       " '1597535youssi',\n",
       " 'tim80327',\n",
       " 'yuri110995',\n",
       " 'n501iomf',\n",
       " 'khurram_',\n",
       " '2021848709.',\n",
       " '2akira2',\n",
       " '2010server',\n",
       " 'sanki1',\n",
       " 'kdl9cl53',\n",
       " '1234159hero',\n",
       " 'aosmaxd0',\n",
       " 'p2share',\n",
       " 'AS0130066',\n",
       " 'wisal1234',\n",
       " 'hisnipes1',\n",
       " 'patri1973',\n",
       " 'exitos2009',\n",
       " '631ihOZogELoVap',\n",
       " 'jonothepoop1',\n",
       " 'walterivl13',\n",
       " 'planes123',\n",
       " 's9830950044',\n",
       " '52558000aaa',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'yzxwvgbdu503',\n",
       " 'kunyukbabi69',\n",
       " '2GnTStTE4Mw4MTwv',\n",
       " 'mustang337',\n",
       " 'xiau5ff',\n",
       " 'bozoxik602',\n",
       " 'parvizrus13',\n",
       " 'denise18',\n",
       " '64whbrb351',\n",
       " '0112358mayin0',\n",
       " 'sasuke4',\n",
       " 'clyioqzgw42',\n",
       " 'ginger972',\n",
       " 'uxyloga692',\n",
       " 'aslpls2009',\n",
       " 'znbl5tj1',\n",
       " 'meriton23',\n",
       " 'yzxwvgbdu503',\n",
       " 'wibi182d',\n",
       " 'uou2dae',\n",
       " 'jbtcnd6',\n",
       " 'a0972986650',\n",
       " 'grazi0201',\n",
       " 'cigicigi123',\n",
       " 'u03kz6ez',\n",
       " 'g3rappa',\n",
       " 'kyxvufl37',\n",
       " 'xp;ysmybst',\n",
       " 'yzxwvgbdu503',\n",
       " 'holamundo1',\n",
       " 'gaymaids1',\n",
       " 'JEQuloqOFUd102',\n",
       " 'moken7',\n",
       " 'oatcake87',\n",
       " '7mV0pKTA3MgHy8Jv',\n",
       " 'mustang337',\n",
       " 'x8512514',\n",
       " 'faranumar91',\n",
       " '1234159hero',\n",
       " 'kayal123',\n",
       " 'xiau5ff',\n",
       " 'peluchin4',\n",
       " 'beijing168',\n",
       " 'as326159',\n",
       " 'd6VyrkFV6oblxNs5N8cW',\n",
       " '1972vishara',\n",
       " 'oioo9og',\n",
       " 'yqugu927',\n",
       " 'tomas7896',\n",
       " 'ass359',\n",
       " 'jsm159167',\n",
       " 'junaid5',\n",
       " 'seng987321',\n",
       " 'VMjz4eTkxNAbOyUU',\n",
       " 'snolyuj04',\n",
       " 'Ju6BIMTU0MwYXtL4',\n",
       " 'asdasdf1',\n",
       " 'RPFUOUDQwMwVW0AS',\n",
       " 'kzde5577',\n",
       " 'mmm23mm',\n",
       " 'DTUQG5jU5MwmR1L9',\n",
       " 'y0unus',\n",
       " '1ngaymuadong',\n",
       " 'c3h8bkzr',\n",
       " 'bugatti01',\n",
       " 'bellsuki1',\n",
       " 'zgmfnwuq25',\n",
       " 'VMjz4eTkxNAbOyUU',\n",
       " 'khurram_',\n",
       " '4osxw4r',\n",
       " 'yami12',\n",
       " '7942vikas',\n",
       " '6975038lp',\n",
       " 'khmer100.03278&?><Mnb',\n",
       " 'a0972986650',\n",
       " 'UF1Z2WjE5Mg26R1K',\n",
       " 'demon10',\n",
       " 'seller1',\n",
       " 'bencike7',\n",
       " 'asv5o9yu',\n",
       " 'terrassa6',\n",
       " 'exitos2009',\n",
       " 'polo2014',\n",
       " 'vgnfs495vp',\n",
       " 'rqmswof2llb0',\n",
       " 'viri13',\n",
       " 'cdann123',\n",
       " 'qwekl12',\n",
       " 'ikanez886',\n",
       " 'tahseen75',\n",
       " 'elabadmin1386',\n",
       " 'killer5',\n",
       " 'v1118714',\n",
       " '746xitEGiqObog',\n",
       " 'go7kew7a2po',\n",
       " 'juliel009',\n",
       " 'a2486315',\n",
       " 'princ3sa',\n",
       " 'jytifok873',\n",
       " 'pazzini24',\n",
       " 'icap12',\n",
       " 'fahad123',\n",
       " 'zoobike04',\n",
       " 'plumilla1',\n",
       " 'yqugu927',\n",
       " '05bumd',\n",
       " 'uziwocy148',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'yogesh143',\n",
       " '6yy6yy',\n",
       " '1qa2ws3ed4rf',\n",
       " 'asgaliu11',\n",
       " 'sono11',\n",
       " 'poseidon2011',\n",
       " 'lymuvop730',\n",
       " 'saule123',\n",
       " '3CgRg8DA1NQY1iEj',\n",
       " 'g3rappa',\n",
       " 'omakiva153',\n",
       " 'sanki1',\n",
       " 'password0880',\n",
       " 'metopelo1623',\n",
       " 'meriton23',\n",
       " 'memjan123',\n",
       " 'ikanez886',\n",
       " 'sd6x9s3s',\n",
       " 'paladinas1',\n",
       " '64959rodro',\n",
       " 'YADHJIGSAWS11',\n",
       " 'vuqADUSatAJO800',\n",
       " '215466kenyi',\n",
       " 'galoucura1',\n",
       " 'gvczfel801',\n",
       " 'fnmsdha476',\n",
       " '1ngaymuadong',\n",
       " 'jerusalem393',\n",
       " 'zcsntdmhe098',\n",
       " 'ycqtgdso3',\n",
       " 'pass0port',\n",
       " 'ram@!sita15392',\n",
       " 'robot425',\n",
       " 'a2531106',\n",
       " '12345yolanda',\n",
       " 'wuzyci421',\n",
       " 'zgmfnwuq25',\n",
       " 'pazzini24',\n",
       " 'pato221182',\n",
       " 'wxS2ztDk4OATjBfI',\n",
       " 'zidadoh258',\n",
       " 'openup12',\n",
       " '9950twofour0',\n",
       " 'carla99',\n",
       " 'd6VyrkFV6oblxNs5N8cW',\n",
       " 'pastorius88',\n",
       " 'gvczfel801',\n",
       " 'pedronha96',\n",
       " 'zgmfnwuq25',\n",
       " 'coy29061994',\n",
       " '10Erjrlmebup0n',\n",
       " 'as326159',\n",
       " 'just1n0k',\n",
       " 'jannia5',\n",
       " 'exitos2009',\n",
       " 'amoadios321',\n",
       " '238wofutUtIGyf',\n",
       " 'tomas7896',\n",
       " 'may112001',\n",
       " 'princ3sa',\n",
       " '600eretz',\n",
       " '12345yolanda',\n",
       " 'marita1',\n",
       " 'olmaz.',\n",
       " 'servbot88',\n",
       " 'virush1n1',\n",
       " 'llahetihw1',\n",
       " 'X9WVojjE4MgVAIiR',\n",
       " 'zjl0kx03',\n",
       " 'lamborghin1',\n",
       " 'X34y2CzY5MACs6kp',\n",
       " 'nK0yKXTU0NQHZE2e',\n",
       " 'fbjurcd961',\n",
       " 'amandine666',\n",
       " '20010509wang',\n",
       " 'jytifok873',\n",
       " 'edcmki90',\n",
       " 'wycinu436',\n",
       " 'amoadios321',\n",
       " 'tin030201',\n",
       " 'ts34a3fodh3i',\n",
       " 'nicolas05',\n",
       " 'cristiano7',\n",
       " 'BsKbJHTY4NgesCOs',\n",
       " 'fbjurcd961',\n",
       " 'gill02',\n",
       " '7mV0pKTA3MgHy8Jv',\n",
       " 'servbot88',\n",
       " '4osxw4r',\n",
       " 'yu4cmn',\n",
       " 'planes123',\n",
       " 'Iamthelegend1!',\n",
       " 'sebax2013',\n",
       " 'novelia21',\n",
       " 'tucagu356',\n",
       " 'laedbchsx687',\n",
       " 'icap12',\n",
       " 'potatobus150',\n",
       " 'koabcswzt3',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'jUV4dSDQwNwPpA36',\n",
       " 'den019520',\n",
       " 'djngeyut2707',\n",
       " 'nLIGyhTU1NQTAp6u',\n",
       " 'afs34214',\n",
       " 'gkrqjs6',\n",
       " 'rLLh4WDQ2OAWbDO5',\n",
       " 'vardhan19',\n",
       " '6975038lp',\n",
       " 'zcsntdmhe098',\n",
       " 'asdasdf1',\n",
       " 'jUV4dSDQwNwPpA36',\n",
       " 'iL1BEmTUyMg8YYbn',\n",
       " 'bc5e4vca',\n",
       " '1A2Z3E4R',\n",
       " '123net123',\n",
       " 'idofo673',\n",
       " 'jonothepoop1',\n",
       " 'witek1709',\n",
       " 'princ3sa',\n",
       " 'nhiannei040',\n",
       " 'mustang337',\n",
       " 'pr0f1s10',\n",
       " '4osxw4r',\n",
       " 'juliel009',\n",
       " 'killer5',\n",
       " '3y6iwef2g6',\n",
       " 'luiskeko31',\n",
       " 'faisal213',\n",
       " 'uqilyni846',\n",
       " 'legna13',\n",
       " 'paulino123',\n",
       " 'pilatyj280',\n",
       " 'ocadezi586',\n",
       " 'paulino123',\n",
       " 'tspirits08',\n",
       " 'yitbos77',\n",
       " 'asgaliu11',\n",
       " 'wbtdrieus345',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'matiofox08',\n",
       " 'elabadmin1386',\n",
       " 'jytifok873',\n",
       " 'universe2908',\n",
       " 'q0pv0fk',\n",
       " '0VKWoODkwOAc0pZK',\n",
       " 'pacific52',\n",
       " 'p2share',\n",
       " 'Oshity07142014',\n",
       " '248sUqiFEJuRag',\n",
       " 'we34dar88',\n",
       " 'seng987321',\n",
       " 'keithcutlip99',\n",
       " 'yut0838828185',\n",
       " 'xanyrum650',\n",
       " 'atigi839',\n",
       " 'ns2b0727',\n",
       " 'kah4544875',\n",
       " 'bgrvl80',\n",
       " 't8IkFRDIxMAFV2JW',\n",
       " 'ikanez886',\n",
       " 't8IkFRDIxMAFV2JW',\n",
       " 'pardalgg5',\n",
       " 'sydney213',\n",
       " 'yllime123',\n",
       " 'sbnivetha123',\n",
       " 'kP82iqDMxNgBMxBP',\n",
       " 'omakiva153',\n",
       " '19821010a',\n",
       " 'snolyuj04',\n",
       " 'BsKbJHTY4NgesCOs',\n",
       " '0847440744z',\n",
       " 'avanakit72',\n",
       " '123456ts',\n",
       " 'alimagik1',\n",
       " 'colorado27',\n",
       " 'oscar69',\n",
       " 'grazi0201',\n",
       " 'cesarmaio1',\n",
       " 'zedika521',\n",
       " 'failz0r',\n",
       " '6975038lp',\n",
       " 'cribrot1200',\n",
       " 'jekkmoeder>',\n",
       " 'webhostv1t1n',\n",
       " 'barra23',\n",
       " '929865yt',\n",
       " '1katertje',\n",
       " 'tin030201',\n",
       " 'wasanun13',\n",
       " 'cigicigi123',\n",
       " 'gill02',\n",
       " 'tim80327',\n",
       " 'gedu1t1ah',\n",
       " 'franczuk33',\n",
       " 'gozv3e5',\n",
       " 'azerty32',\n",
       " 'ekufite742',\n",
       " 'rLLh4WDQ2OAWbDO5',\n",
       " 'kzde5577',\n",
       " 'kswa2mrv',\n",
       " 'zb08110229',\n",
       " 'bellsuki1',\n",
       " 'kukimuki123',\n",
       " 'Truelove19902610',\n",
       " 'ym2130104',\n",
       " 'atigi839',\n",
       " 'webhost08',\n",
       " '2fakjv',\n",
       " 'intel1',\n",
       " '2652033abc',\n",
       " 'mega0109',\n",
       " 'oekojWyH120063',\n",
       " 'satrjcrj6',\n",
       " 'lymuvop730',\n",
       " '01161590m',\n",
       " 'oekojWyH120063',\n",
       " '929865yt',\n",
       " 'pacman23',\n",
       " 'la3na4you',\n",
       " 'cdann123',\n",
       " 'lrhxmevb620',\n",
       " 'hlQ8gDTExMQWkeda',\n",
       " 'icap12',\n",
       " 'smart95',\n",
       " 'mickael12',\n",
       " '19840510kkk1',\n",
       " 'legna13',\n",
       " 'hide68',\n",
       " 'franczuk33',\n",
       " 'asv5o9yu',\n",
       " 'IjUcOtYqAwel725',\n",
       " 'njmania114',\n",
       " 'qefoquf1uf',\n",
       " 'pizxmwaos537',\n",
       " 'eth36498',\n",
       " '123477889a',\n",
       " 'schalke04',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'taurofive16',\n",
       " 'oekojWyH120063',\n",
       " 'Jovan13lovekenthjusvan4ever',\n",
       " 'Scipio21152030067254',\n",
       " 'a1233210',\n",
       " 'barboza221294',\n",
       " '0169395484a',\n",
       " 'parvizrus13',\n",
       " 'poluxyj32',\n",
       " 'kunyukbabi69',\n",
       " 'ok>bdk',\n",
       " 'tPGMkBjkyMg3hGzu',\n",
       " 'sw10d014',\n",
       " 'wjngzro27',\n",
       " 'aquhih220',\n",
       " 'mickael12',\n",
       " 'cUFUSYKIPuGo024',\n",
       " 'mickael12',\n",
       " 'berserk18',\n",
       " 'olmaz.',\n",
       " 'djda1203zj',\n",
       " 'wjngzro27',\n",
       " '123net123',\n",
       " 'virush1n1',\n",
       " 'pedronha96',\n",
       " '1597535youssi',\n",
       " 'khurram_',\n",
       " '2d0d7qfz',\n",
       " 'tahseen75',\n",
       " 'w1ll1ams',\n",
       " 'aan2900220',\n",
       " 'adfbwrnt4',\n",
       " 'ass359',\n",
       " '238wofutUtIGyf',\n",
       " 'mohantra1',\n",
       " '631ihOZogELoVap',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'GGmm26120904..',\n",
       " 'xf6385494',\n",
       " 'atigi839',\n",
       " 'franczuk33',\n",
       " 'hayhayq2',\n",
       " 'gpc151192',\n",
       " 'c3h8bkzr',\n",
       " 'aslpls2009',\n",
       " 'DRAGON25',\n",
       " 'brokensong54',\n",
       " 'vocal0619',\n",
       " 'z3ro1sm',\n",
       " 'taccy12',\n",
       " 'lsdlsd1',\n",
       " '929865yt',\n",
       " '33kanun03',\n",
       " 'ekufite742',\n",
       " 'medebizu3',\n",
       " 'luthien123',\n",
       " 'khurram_',\n",
       " 'okn9zp9o',\n",
       " '3CgRg8DA1NQY1iEj',\n",
       " 'afs34214',\n",
       " 'deivid1991',\n",
       " 'kdl9cl53',\n",
       " 'speedracer10',\n",
       " 'holamundo1',\n",
       " 'witek1709',\n",
       " 'mathilde54550',\n",
       " 'bghuyku37',\n",
       " 'aio42fv',\n",
       " 'mmm23mm',\n",
       " 'atigi839',\n",
       " 'just1n0k',\n",
       " '20Dgw7TQ0OQVdly7',\n",
       " '123456ts',\n",
       " 'xf6385494',\n",
       " 'kukimuki123',\n",
       " 'h1h2h3h4h5',\n",
       " 'ras996633',\n",
       " 'spl51190595',\n",
       " 'pekai2004',\n",
       " 'acgyj188',\n",
       " 'ewvjbilan4',\n",
       " 'gkrqjs6',\n",
       " 'taulant123',\n",
       " 'vuqADUSatAJO800',\n",
       " 'zeeshanbhai1',\n",
       " 'naseKoBUMIg295',\n",
       " 'franczuk33',\n",
       " 'brokensong54',\n",
       " 'wuzyci421',\n",
       " 'RqsuUsDYxNgr8T40',\n",
       " 'numero2',\n",
       " 'nhfdff2512',\n",
       " '241189dumai',\n",
       " 'ryjypes139',\n",
       " '3rambywople',\n",
       " '0123one47',\n",
       " 'www32223222',\n",
       " 'sgtg5fq',\n",
       " 'w1e2s3l4',\n",
       " 'BZVQZBTM1MApRV7s',\n",
       " 'icap12',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'zeeshanbhai1',\n",
       " 'finisterra1',\n",
       " 'xie635891',\n",
       " 'ewvjbilan4',\n",
       " 'byeypb2',\n",
       " 'xzeyfbi495',\n",
       " '26522876p',\n",
       " 'gozv3e5',\n",
       " 'obstacle25',\n",
       " 'mario489800',\n",
       " 'avanakit72',\n",
       " 'jonothepoop1',\n",
       " 'cigicigi123',\n",
       " 'Truelove19902610',\n",
       " 'pikey231',\n",
       " 'clave08',\n",
       " 'woaini0',\n",
       " 'alodise603',\n",
       " '20010509wang',\n",
       " '123456rajput',\n",
       " 'mickael12',\n",
       " 'den019520',\n",
       " 'mialr325',\n",
       " 'koulapic33',\n",
       " 'kzde5577',\n",
       " 't8IkFRDIxMAFV2JW',\n",
       " 'fudijep286',\n",
       " 'diarie1',\n",
       " '7942vikas',\n",
       " 'vehat387',\n",
       " 'xW8-3w7-MFB-CKH',\n",
       " 'hodaq103',\n",
       " 'Oshity07142014',\n",
       " 'autan88',\n",
       " 'frhnsvelhfr1',\n",
       " 'ilonu497',\n",
       " 'hodygid757',\n",
       " 'brokensong54',\n",
       " 'lymuvop730',\n",
       " 'webhostv1t1n',\n",
       " 'eVl19ADIxNAmU09N',\n",
       " 'bafiqkxwu0',\n",
       " 'Ju6BIMTU0MwYXtL4',\n",
       " 'tukaxo486',\n",
       " 'cribrot1200',\n",
       " 'abizar08',\n",
       " 'upyjlneg80',\n",
       " 'schalke04',\n",
       " 'hamqrc6',\n",
       " 'kayal123',\n",
       " 'xp;ysmybst',\n",
       " 'openup12',\n",
       " 'olmaz.',\n",
       " 'paladinas1',\n",
       " 'ufoduvo540',\n",
       " 'webhost0hm000',\n",
       " 'gracimir87',\n",
       " 'klister1',\n",
       " '2021848709.',\n",
       " 'snolyuj04',\n",
       " 'nelva20',\n",
       " 'taiga0088',\n",
       " '1994kerr1994',\n",
       " 'trabajonet9',\n",
       " '12345687vini',\n",
       " 'bijou2012',\n",
       " 'IjUcOtYqAwel725',\n",
       " 'uqilyni846',\n",
       " 'mikrochip0',\n",
       " 'r0cker',\n",
       " 'znbl5tj1',\n",
       " '123maxbala',\n",
       " 'xawipy995',\n",
       " 'ilonu497',\n",
       " '01161590m',\n",
       " 'yogesh143',\n",
       " 'SLAEgyTk0OQxphJq',\n",
       " 'ubojig109',\n",
       " 'yu86640132',\n",
       " 'qopybuxi2',\n",
       " 'z888888',\n",
       " 'macias2010',\n",
       " 'la3na4you',\n",
       " 'rsuvxz08b',\n",
       " 'ebacuro434',\n",
       " 'ass359',\n",
       " 'amoadios321',\n",
       " 'satelite31',\n",
       " '20Dgw7TQ0OQVdly7',\n",
       " 'papasito1991',\n",
       " 'lolmdr1',\n",
       " '33kanun03',\n",
       " 'nikolas369',\n",
       " 'alimagik1',\n",
       " 'oscar69',\n",
       " 'vuqADUSatAJO800',\n",
       " 'teste10',\n",
       " 'cribrot1200',\n",
       " 'intel1',\n",
       " 'RRaa--72783530081984',\n",
       " 'juanito00',\n",
       " 'aio42fv',\n",
       " 'mialr325',\n",
       " '358gohappy',\n",
       " 'cesarmaio1',\n",
       " 'wjngzro27',\n",
       " 's0xwym7h',\n",
       " 'kayal123',\n",
       " 'ixehawojEPe418',\n",
       " 'alodise603',\n",
       " 'VMjz4eTkxNAbOyUU',\n",
       " 'amandine666',\n",
       " 'igejasy712',\n",
       " 'caitianci33',\n",
       " 'gpc151192',\n",
       " 'paynu4pzwzw11',\n",
       " 'hqh2eYjQxOQPYIsA',\n",
       " 'xknagqo92',\n",
       " '64959rodro',\n",
       " 'mayur@8netinfotech',\n",
       " 'sarahi1628',\n",
       " 'josue12',\n",
       " 'daylit9',\n",
       " 'ukyzopi369',\n",
       " 'shakes@000webhost',\n",
       " 'snolyuj04',\n",
       " 'patata91',\n",
       " 'sarahi1628',\n",
       " 'ns2b0727',\n",
       " 'seng987321',\n",
       " 'paola1995',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x # printing x list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y # printing y list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW WE NEED TO APPLY TF-IDF(TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY) OF DATA\n",
    "# creating a custom function to split the word into characters\n",
    "def word_divide_char(inputs):\n",
    "    character=[]\n",
    "    for i in inputs:\n",
    "        character.append(i)\n",
    "    return character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'z', 'd', 'e', '5', '5', '7', '7']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_divide_char('kzde5577')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we import TF-IDF vectorizer to convert String data into numerical data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(tokenizer=word_divide_char) # we tokenize the data on the basis of word_divide_char functon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply TF-IDF vectorizer on data, x(all passwords)\n",
    "X=vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(669639, 127)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # second column size increased because it is now vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x05',\n",
       " '\\x06',\n",
       " '\\x08',\n",
       " '\\x0f',\n",
       " '\\x10',\n",
       " '\\x11',\n",
       " '\\x12',\n",
       " '\\x13',\n",
       " '\\x17',\n",
       " '\\x19',\n",
       " '\\x1b',\n",
       " '\\x1c',\n",
       " '\\x1d',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '\\xa0',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names() # getting all the features of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x127 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_document_vector=X[0]\n",
    "first_document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.56773594],\n",
       "        [0.        ],\n",
       "        [0.59062276],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.28588741],\n",
       "        [0.22117119],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.29203864],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.33601457],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_document_vector.T.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.590623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.567736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.336015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>0.292039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.285887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>;</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TF-IDF\n",
       "7   0.590623\n",
       "5   0.567736\n",
       "z   0.336015\n",
       "k   0.292039\n",
       "d   0.285887\n",
       "..       ...\n",
       "<   0.000000\n",
       ";   0.000000\n",
       "9   0.000000\n",
       "8   0.000000\n",
       "   0.000000\n",
       "\n",
       "[127 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to prepare the data for modelling purpose with first column as feature, and second column as the importance of that feature\n",
    "# this is final data for modelling purpose\n",
    "df=pd.DataFrame(first_document_vector.T.todense(),index=vectorizer.get_feature_names(),columns=['TF-IDF'])\n",
    "df.sort_values(by=['TF-IDF'],ascending=False) # arranged the data in decreaing order of TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to pass this data for modelling purpose (Applying Machine Learning)\n",
    "# first we need to split the data for training and testing purpose\n",
    "# train - To learn the relationship within data, \n",
    "# test - To do predictions, and this testing data will be unseen to my model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2) # using train_test_split, we splitted the data, train - 80% of data, test - 20% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535711, 127)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Creation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now after splitting our data is ready for modelling stuff\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(random_state=0,multi_class='multinomial') # we consider case of multinomial logistic regression, because we have three types of password - 0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', random_state=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train) # fitting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have trained our data, so we consider some rare case which is not in data, and predicts its strength\n",
    "dt=np.array(['%@123abcd'])\n",
    "pred=vectorizer.transform(dt)\n",
    "clf.predict(pred) # returned 1 there fore password is of average strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same way we can do prediction on X test data, In the same we can also consider some of the advanced classifier, such as adabboost, catboost, randomforest\n",
    "y_pred=clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the LinearRegression model comes to be: \n",
      " \n",
      "0.8199738291728189\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the LinearRegression model comes to be: \\n \") \n",
    "print(clf.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing model\n",
    "from sklearn.linear_model import Ridge\n",
    "reg2 = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting data into the model.\n",
    "reg2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65036879, 1.34446634, 0.66358794, ..., 1.16026241, 0.93617659,\n",
       "       1.00789623])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions \n",
    "pred2 = reg2.predict(X_test)\n",
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the RidgeRegression model comes to be: \n",
      " \n",
      "0.43772257768430856\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the RidgeRegression model comes to be: \\n \") \n",
    "print(reg2.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decision Tree Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing decision tree regressor \n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "dec = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting data into the model.\n",
    "dec.fit(X_train, y_train)\n",
    "# Making predictions on Test data \n",
    "pred3 = dec.predict(X_test)\n",
    "pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree Regressor  model comes to be: \n",
      " \n",
      "0.9996337631007151\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the Decision Tree Regressor  model comes to be: \\n \") \n",
    "print(dec.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Performance Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\t\t\t RootMeanSquareError \t\t Accuracy of the model\n",
      "Linear Regression \t\t 0.4271 \t \t\t 0.8200\n",
      "Ridge Regression \t\t 0.3820 \t \t\t 0.4377\n",
      "Decision Tree Regressor\t\t 0.1731 \t \t\t 0.9996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Model\\t\\t\\t RootMeanSquareError \\t\\t Accuracy of the model\") \n",
    "print(\"\"\"Linear Regression \\t\\t {:.4f} \\t \\t\\t {:.4f}\"\"\".format(  np.sqrt(mean_squared_error(y_test, y_pred)), clf.score(X_train,y_train)))\n",
    "print(\"\"\"Ridge Regression \\t\\t {:.4f} \\t \\t\\t {:.4f}\"\"\".format(  np.sqrt(mean_squared_error(y_test, pred2)), reg2.score(X_train,y_train)))\n",
    "print(\"\"\"Decision Tree Regressor\\t\\t {:.4f} \\t \\t\\t {:.4f}\"\"\".format(  np.sqrt(mean_squared_error(y_test, pred3)), dec.score(X_train,y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of decision tree regressor is higher and root mean sqaure error is least.\n",
    "\n",
    "Thus, Decision tree regressor is more efficient model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
